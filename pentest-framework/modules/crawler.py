"""
Módulo de web crawling usando katana, waybackurls, gau.
"""

from typing import Dict, Any, List
from core.tool_manager import ToolManager
from core.colors import Colors


class WebCrawler:
    """Web crawler que usa múltiplas fontes"""
    
    def __init__(self, tool_manager: ToolManager):
        self.tm = tool_manager
        self.results = {
            'urls': [],
            'parameters': [],
            'js_files': [],
            'api_endpoints': []
        }
    
    def scan(self, target: str) -> Dict[str, Any]:
        """Executa crawling completo"""
        print(f"\n{Colors.BLUE}[*] Crawling...{Colors.RESET}")
        
        all_urls = set()
        domain = self._extract_domain(target)
        
        # 1. Katana - Active crawling
        print(f"{Colors.YELLOW}  ├── Katana (active crawling)...{Colors.RESET}")
        katana_urls = self.tm.run_katana(target, depth=2, js_crawl=True)
        all_urls.update(katana_urls)
        print(f"{Colors.GREEN}  ├── URLs encontradas: {len(katana_urls)}{Colors.RESET}")
        
        # 2. Waybackurls - Historical URLs
        print(f"{Colors.YELLOW}  ├── Waybackurls (historical)...{Colors.RESET}")
        wayback_urls = self.tm.run_waybackurls(domain)
        all_urls.update(wayback_urls)
        print(f"{Colors.GREEN}  ├── URLs históricas: {len(wayback_urls)}{Colors.RESET}")
        
        # 3. GAU - GetAllURLs
        print(f"{Colors.YELLOW}  ├── GAU (aggregate)...{Colors.RESET}")
        gau_urls = self.tm.run_gau(domain)
        all_urls.update(gau_urls)
        print(f"{Colors.GREEN}  ├── URLs do GAU: {len(gau_urls)}{Colors.RESET}")
        
        # Processar URLs
        self.results['urls'] = list(all_urls)
        
        # Identificar tipos especiais
        for url in self.results['urls']:
            if '?' in url:
                self.results['parameters'].append(url)
            if url.endswith('.js'):
                self.results['js_files'].append(url)
            if '/api/' in url or '/v1/' in url or '/v2/' in url:
                self.results['api_endpoints'].append(url)
        
        print(f"{Colors.GREEN}  └── Total de URLs únicas: {len(self.results['urls'])}{Colors.RESET}")
        print(f"{Colors.CYAN}     ├── Com parâmetros: {len(self.results['parameters'])}{Colors.RESET}")
        print(f"{Colors.CYAN}     ├── JavaScript: {len(self.results['js_files'])}{Colors.RESET}")
        print(f"{Colors.CYAN}     └── API endpoints: {len(self.results['api_endpoints'])}{Colors.RESET}")
        
        return self.results
    
    def _extract_domain(self, url: str) -> str:
        """Extrai domínio da URL"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        return parsed.netloc or parsed.path

